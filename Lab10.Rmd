---
title: "Lab10"
author: "Erika Bueno"
date: "October 31, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r}
##1)Using a for loop, write a function to calculate the number of zeroes in a numeric vector. Before entering the loop, set up a counter variable counter <- 0. Inside the loop, add 1 to counter each time you have a zero in the matrix. Finally, use return(counter) for the output.
z<- c(0,0,0,0,0,0,1)
counts<-function(a=z){
counter<-0 
for (i in seq_along(a)) {
   if (a[i]==0) counter<- 1 + counter
   }
return(counter)}

counts()
```

```{r}
##2)Use subsetting instead of a loop to rewrite the function as a single line of code.
zeros<- function(a=z){
  subset<-length(a[a==0])
return(subset)}

zeros()
```

```{r}
##3)Write a function that takes as input two integers representing the number of #rows and columns in a matrix. The output is a matrix of these dimensions in #which each element is the product of the row number x the column number.

mat<- matrix(1:10, nrow=2, ncol = 5)
func1<- function(x=mat){
for (i in 1:nrow(x)) {
  for (j in 1:ncol(x)){
    newmatrix<- matrix(x[i,j] * i *j, ncol = 5, nrow=2)
    return(newmatrix)
    }}} 
func1()
mat



func2<- function(numrow= 4, numcol=5){
  mat2<- matrix(nrow=numrow, ncol=numcol)
  for (i in 1:nrow(mat2)) {
  for (j in 1:ncol(mat2)){
        mat2[i,j]<- i * j
  }}
          return(mat2)
}

func2(2,5) 
```
```{r warning=F, message=F}
##4) Use the code from yesterday's class to design and conduct a randomization test #for some of your own data. You will need to modify the functions that read in #the data, calculate the metric, and randomize the data. Once those are set up, #the program should run correctly calling your new functions. Also, to make your #analysis fully repeatable, make sure you set the random number seed at the #beginning (use either set.seed() in base R, or char2seed in the TeachingDemos #package

#reading in a simulated dataset 

readData <- function(z=NULL) {
  if (is.null(z)) {
    response <- round(runif(36, min= 30, max= 85),0) 
    treatment <- rep(c("a", "b","c","d","e","f"), each=6) # 6 treatment groups
    dFrame<-data.frame(ID=seq_along(response), response,treatment, row.names = NULL, stringsAsFactors = TRUE) #r wont let me knit 
    return(dFrame)
    }
    }
readData()
dFrame<-readData()


# performing and analysis of variance 
library(dplyr)
library(broom)
getMetric <- function(z=NULL) {
  . <- aov(dFrame[,2]~dFrame[,3])  #need to turn numbers into letter groups
  .<- tidy(.)
  .<- .$p.value  #extracting p-value from summary aov
  return(.)
}

getMetric() #p-value: 0.44> 0.05
pvalue=0.328
# randomize 2nd column == response

shuffleData <- function(z=NULL) {
  dFrame[,2] <- sample(dFrame[,2])
  return(dFrame[,2])
}
shuffleData()

# get p value

getPvalue <- function(z=NULL) {
  pLower <- mean(dFrame[[2]] <= dFrame[[1]]) 
  pUpper <- mean(dFrame[[2]] >= dFrame[[1]])
  return(c(pLower,pUpper))
}

getPvalue()


# histogram of simulated values
library(ggplot2)
plotRanTest <- function(z=NULL) {
  dF <- data.frame(ID=seq_along(dFrame[[2]]),simX=dFrame[[2]])
  p1 <- ggplot(data=dF,mapping=aes(x=simX))
  p1 + geom_histogram(mapping=aes(fill=I("goldenrod"),color=I("black"))) +
    geom_vline(aes(xintercept=1,col="blue")) 
  
}
plotRanTest()

# perform randomization test
nSim <- 100
Xsim <- rep(NA, nSim)
dF <- readData() 
Xobs <- getMetric(dF)

for (i in seq_len(nSim)) {
  Xsim[i] <- getMetric(shuffleData(dF))
}

pvals <- list(Xobs,Xsim)
getPvalue(pvals) 
plotRanTest(pvals)
```
```{r}
##5)For comparison, calculate in R the standard statistical analysis you would use with these data. How does the p-value compare for the standard test versus the p value you estimated from your randomization test? If the p values seem very different, run the program again with a different starting seed (and/or increase the number of replications in your randomization test). If there are persistent differences in the p value of the standard test versus your randomization, what do you think is responsible for this difference?

beetles<- read.csv("CPBWeights.csv") #reading in real data of beetle weights
head(beetles)
Tgroup<- as.factor(beetles$Treatment) #setting my treatment groups as factors
Anova<-aov(FinalWeight~Tgroup, beetles) #running the anova model against Final.Weight
summary.aov(Anova) #summary stats of the model
library(broom)
tidy(Anova)$p.value #p-value = 0.483 which falls in between the upper and lower p-values generated by my randomization test [ 0 1.00000000]

#after running the program with a different starting seed, I still get p-values> 0.05 suggesting that the non-significance of my observed metric is not due to chance alone. Running multiple randomization tests continued to generate high p-values.
 #this package allows you to place p-values on graphs
library(ggpubr)
library(wesanderson)
p <- ggplot(beetles,mapping =  aes(x=Treatment, y=FinalWeight)) + 
  geom_boxplot() +
  labs(title="Beetle Weights (mg)",
       subtitle = "Final weights of beetles according to treatment group",
       x="Treatment",
       y="Mass (mg)")
p1<-p+ aes(fill=Treatment) + scale_fill_manual(values= wes_palette("Cavalcanti1", 6, type="continuous"))
p1 + stat_compare_means(method = "anova") 

```

